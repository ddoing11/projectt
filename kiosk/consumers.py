import json
from channels.generic.websocket import AsyncWebsocketConsumer
import azure.cognitiveservices.speech as speechsdk
import asyncio
from django.conf import settings
from asgiref.sync import async_to_sync
import wave

# ì´ í•¨ìˆ˜ëŠ” ì´ë¯¸ ìœ„ìª½ì—ì„œ ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì„í¬íŠ¸í•  í•„ìš” ì—†ìŒ
def save_wav_from_bytes(audio_bytes, file_path):
    with wave.open(file_path, 'wb') as wf:
        wf.setnchannels(1)            # mono
        wf.setsampwidth(2)            # 16-bit
        wf.setframerate(16000)        # 16 kHz
        wf.writeframes(audio_bytes)

class AzureSTTConsumer(AsyncWebsocketConsumer):
    async def connect(self):
        await self.accept()
        self.buffer = bytearray()
        self.recognizing = False
        self.loop = asyncio.get_event_loop()
        print("ğŸ¤ WebSocket ì—°ê²°ë¨")

    async def disconnect(self, close_code):
        print("âŒ ì—°ê²° ì¢…ë£Œ")
        self.recognizing = False

    async def receive(self, text_data=None, bytes_data=None):
        if bytes_data:
            self.buffer.extend(bytes_data)

            # ì•½ 3ì´ˆ ì´ìƒ ìŒì„± ë°ì´í„°ê°€ ëª¨ì˜€ì„ ë•Œë§Œ ì²˜ë¦¬
            if len(self.buffer) > 16000 * 2 and not self.recognizing:
                self.recognizing = True
                audio_bytes = bytes(self.buffer)
                self.buffer = bytearray()

                await self.loop.run_in_executor(None, self.recognize_and_send, audio_bytes)
                self.recognizing = False

    # ì´ ì•ˆì—ì„œ Azure ìŒì„± ì¸ì‹ ì²˜ë¦¬
    def recognize_and_send(self, audio_bytes):
        import tempfile

        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            save_wav_from_bytes(audio_bytes, tmp.name)
            tmp_path = tmp.name

        speech_config = speechsdk.SpeechConfig(
            subscription=settings.AZURE_SPEECH_KEY,
            region=settings.AZURE_SPEECH_REGION
        )
        audio_config = speechsdk.AudioConfig(filename=tmp_path)
        recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)

        result = recognizer.recognize_once()
        print("ğŸ“ ì¸ì‹ ê²°ê³¼:", result.text)

        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            async_to_sync(self.send)(text_data=json.dumps({'text': result.text}))
